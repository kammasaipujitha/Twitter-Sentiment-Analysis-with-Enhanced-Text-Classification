
#1.importing libraries
"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

#printing the stopwords in english language
print(stopwords.words('english'))

"""#2.preprocessing

1.framing the data into data frame
"""

#loading the data from csv file to pandas data frame
twitter_data = pd.read_csv('/content/twitter_training.csv',encoding= 'ISO-8859-1')

#printing the no of rows and columns
twitter_data.shape

#printing the first five rows
twitter_data.head()

"""2.data managing"""

#naming the columns
column_names=['id','flag','target','text']
twitter_data = pd.read_csv('/content/twitter_training.csv', names = column_names, encoding= 'ISO-8859-1')

twitter_data.shape

twitter_data.head()

#counting the no of missing values
twitter_data.isnull().sum()

twitter_data=twitter_data.dropna()
twitter_data.isna().sum()

#checking the distribution of target column
twitter_data['target'].value_counts()

"""3.stemming

"""

port_stem = PorterStemmer()

def preprocess_and_tokenize(content):
    # Removing non-alphabet characters and converting to lowercase
    content = re.sub('[^a-zA-Z]', ' ', content).lower()
    # Splitting and removing stopwords
    tokens = [port_stem.stem(word) for word in content.split() if word not in stopwords.words('english')]
    return tokens

# Tokenize the tweets
twitter_data['tokens'] = twitter_data['text'].apply(preprocess_and_tokenize)

from sklearn.preprocessing import LabelEncoder
# Assuming your target column is named 'target'
label_encoder = LabelEncoder()
twitter_data['target'] = label_encoder.fit_transform(twitter_data['target'])

# Check the unique values after encoding
print("Encoded classes:", label_encoder.classes_)

# Separating the data and labels
x = twitter_data['tokens'].values
y = twitter_data['target'].values

twitter_data.head()

print(x)

print(y)

"""#3.Splitting the data into training data and test data


"""

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,stratify=y,random_state=2)

print(x.shape,x_train.shape,x_test.shape)

"""4.vectorization

"""

import gensim
from gensim.models import Word2Vec

#5. Word2Vec Embeddings
# Training Word2Vec model on the training data
w2v_model = Word2Vec(sentences=x_train.tolist(), vector_size=100, window=5, min_count=1, sg=1)

# Function to get the average word vector for each tweet
def get_average_word2vec(tokens, model, vector_size):
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    if len(vectors) > 0:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(vector_size)

# Creating average word vectors for each tweet in training and test sets
x_train_vec = np.array([get_average_word2vec(tokens, w2v_model, 100) for tokens in x_train])
x_test_vec = np.array([get_average_word2vec(tokens, w2v_model, 100) for tokens in x_test])

"""# 4.Training the Machine learning model

Logistic Regression
"""

lr = LogisticRegression()
lr.fit(x_train_vec,y_train)

x_train_prediction = lr.predict(x_train_vec)
training_data_accuracy = accuracy_score(x_train_prediction,y_train)

print('Accuracy score of the training data: ',training_data_accuracy*100)

import xgboost as xgb
clf_xgb=xgb.XGBClassifier()
clf_xgb.fit(x_train_vec,y_train)
x_train_prediction_xgb=clf_xgb.predict(x_train_vec)
training_data_accuracy_xgb=accuracy_score(x_train_prediction_xgb,y_train)
print(training_data_accuracy_xgb*100)

from sklearn.ensemble import RandomForestClassifier
#random forest model
rf_model = RandomForestClassifier()
rf_model.fit(x_train_vec, y_train)

x_train_prediction_rf = rf_model.predict(x_train_vec)
accuracy_rf = accuracy_score(y_train, x_train_prediction_rf)
#print(f'Accuracy of Random Forest model: {accuracy_rf:.2f}')
print(accuracy_rf*100)

from sklearn.preprocessing import MinMaxScaler

# Assuming x_train_vec is your feature matrix with potential negative values
scaler = MinMaxScaler()
x_train_vec_scaled = scaler.fit_transform(x_train_vec)
x_test_vec_scaled = scaler.transform(x_test_vec)

"""MN Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
# Now fit the MultinomialNB model
classifier = MultinomialNB()
classifier.fit(x_train_vec_scaled, y_train)

x_train_prediction_nb = classifier.predict(x_train_vec_scaled)
training_datas_accuracy_nb = accuracy_score(x_train_prediction_nb,y_train)

print('Accuracy score of the training data: ',training_datas_accuracy_nb*100)

from sklearn.ensemble import VotingClassifier
ensemble_model = VotingClassifier(estimators=[
    ('logistic_regression', lr),
    ('xgboost', clf_xgb),
    ('naive_bayes', classifier),
    ('randomforest', rf_model)
], voting='hard')  # Use 'soft' for probability-based voting

# Train the ensemble model on the training data
ensemble_model.fit(x_train_vec_scaled, y_train)

# Evaluate the ensemble model on training data
y_train_pred_ensemble = ensemble_model.predict(x_train_vec_scaled)
training_accuracy_ensemble = accuracy_score(y_train_pred_ensemble, y_train)
print('Ensemble Model Accuracy on Training Data: ', training_accuracy_ensemble * 100)

# Plotting training accuracies

import seaborn as sns
import matplotlib.pyplot as plt

models = ['Logistic Regression', 'XGBoost', 'Naive Bayes', 'RandomForest']
accuracies = [
    training_data_accuracy * 100,
    training_data_accuracy_xgb * 100,
    training_datas_accuracy_nb * 100,
    accuracy_rf * 100
]

plt.figure(figsize=(10, 6))
sns.barplot(x=models, y=accuracies)
plt.title('Training Accuracy of Different Models')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)
plt.show()
